2025-09-10 16:07:50 - DEBUG - Starting query module initialization...
2025-09-10 16:07:50 - DEBUG - Added project root to Python path: C:\Users\dlaev\scaffold\scaffold_ai
2025-09-10 16:07:50 - DEBUG - Importing configuration...
2025-09-10 16:07:50 - INFO - Using standard model (no ONNX): TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-09-10 16:07:50 - INFO - \U0001f680 GPU detected: NVIDIA GeForce RTX 2080 Ti with 11.8GB VRAM
2025-09-10 16:07:50 - DEBUG - Configuration imported successfully
2025-09-10 16:07:50 - DEBUG - Importing LLM manager...
C:\Users\dlaev\scaffold\scaffold_ai\scaffold_env_312_py31210\Lib\site-packages\torch\onnx\_internal\registration.py:168: OnnxExporterWarning: Symbolic function 'aten::scaled_dot_product_attention' already registered for opset 14. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.
  warnings.warn(
2025-09-10 16:07:50 - DEBUG - CUDA available - using GPU optimizations
2025-09-10 16:07:50 - DEBUG - Starting LLM module initialization...
2025-09-10 16:07:50 - DEBUG - Added project root to Python path: C:\Users\dlaev\scaffold\scaffold_ai
2025-09-10 16:07:50 - DEBUG - Importing configuration...
2025-09-10 16:07:50 - DEBUG - Configuration imported successfully
2025-09-10 16:07:50 - DEBUG - LLM manager imported successfully
2025-09-10 16:07:51 - INFO - Initializing improved enhanced query system...
2025-09-10 16:07:51 - INFO - Use pytorch device_name: cuda:0
2025-09-10 16:07:51 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-09-10 16:07:51 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-09-10 16:07:51 - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2025-09-10 16:07:51 - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2025-09-10 16:07:51 - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2025-09-10 16:07:51 - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2025-09-10 16:07:51 - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2025-09-10 16:07:51 - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2025-09-10 16:07:51 - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2025-09-10 16:07:51 - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2025-09-10 16:07:51 - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2025-09-10 16:07:51 - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2025-09-10 16:07:51 - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2025-09-10 16:07:51 - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2025-09-10 16:07:51 - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2025-09-10 16:07:51 - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-09-10 16:07:51 - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2025-09-10 16:07:53 - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-09-10 16:07:53 - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2025-09-10 16:07:53 - DEBUG - https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2025-09-10 16:07:53 - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2025-09-10 16:07:53 - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2025-09-10 16:07:53 - DEBUG - https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6861
2025-09-10 16:07:54 - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-09-10 16:07:54 - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-09-10 16:07:54 - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/config.json HTTP/1.1" 200 0
2025-09-10 16:07:54 - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-09-10 16:07:54 - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-09-10 16:07:54 - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/tokenizer_config.json HTTP/1.1" 200 0
2025-09-10 16:07:54 - DEBUG - https://huggingface.co:443 "GET /api/models/cross-encoder/ms-marco-MiniLM-L-6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 307 147
2025-09-10 16:07:54 - DEBUG - https://huggingface.co:443 "GET /api/models/cross-encoder/ms-marco-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2025-09-10 16:07:54 - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/README.md HTTP/1.1" 307 0
2025-09-10 16:07:54 - DEBUG - https://huggingface.co:443 "HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2025-09-10 16:07:54 - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/cross-encoder/ms-marco-MiniLM-L6-v2/c5ee24cb16019beea0893ab7796b1df96625c6b8/README.md HTTP/1.1" 200 0
2025-09-10 16:07:54 - INFO - Use pytorch device: cuda:0
2025-09-10 16:07:55 - DEBUG - https://huggingface.co:443 "GET /api/models/cross-encoder/ms-marco-MiniLM-L-6-v2 HTTP/1.1" 307 82
2025-09-10 16:07:55 - DEBUG - https://huggingface.co:443 "GET /api/models/cross-encoder/ms-marco-MiniLM-L6-v2 HTTP/1.1" 200 5261
2025-09-10 16:07:55 - INFO - Loaded 2901 metadata entries from C:\Users\dlaev\scaffold\scaffold_ai\vector_outputs\scaffold_metadata_1.json
2025-09-10 16:07:55 - INFO - \u2705 Enhanced query system initialized successfully
2025-09-10 16:07:55 - INFO -    - FAISS index: 2901 vectors
2025-09-10 16:07:55 - INFO -    - Metadata: 2901 entries
2025-09-10 16:07:55 - INFO - Processing query: What is sustainability?
=== TESTING WITH TINYLLAMA MODEL ===

Query: What is sustainability?
========================================
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|##########| 1/1 [00:00<00:00,  2.03it/s]Batches: 100%|##########| 1/1 [00:00<00:00,  2.03it/s]
2025-09-10 16:07:56 - DEBUG - Found 50 initial candidates
2025-09-10 16:07:56 - DEBUG - Simple/short query detected: What is sustainability?. Skipping cross-encoder reranking.
2025-09-10 16:07:56 - DEBUG - Reranked to 3 candidates
2025-09-10 16:07:56 - DEBUG - Contextual filtering: 3 candidates remaining after filtering
2025-09-10 16:07:56 - DEBUG - Filtered to 3 candidates
2025-09-10 16:07:56 - DEBUG - Using top 3 filtered candidates
2025-09-10 16:07:56 - DEBUG - Generated prompt with ~386 tokens
2025-09-10 16:07:56 - DEBUG - Initializing LLM Manager with model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
2025-09-10 16:07:56 - DEBUG - Using device: cuda
2025-09-10 16:07:56 - DEBUG - Using cache directory: C:\Users\dlaev/.cache/huggingface/hub
2025-09-10 16:07:56 - DEBUG - Loading tokenizer...
2025-09-10 16:07:57 - DEBUG - Tokenizer loaded successfully
2025-09-10 16:07:57 - DEBUG - Loading model using standard pipeline approach...
2025-09-10 16:07:57 - INFO - \U0001f680 Using GPU-optimized pipeline settings (no quantization)
2025-09-10 16:07:57 - DEBUG - https://huggingface.co:443 "HEAD /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json HTTP/1.1" 401 0
2025-09-10 16:07:59 - DEBUG - https://huggingface.co:443 "HEAD /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/generation_config.json HTTP/1.1" 401 0
2025-09-10 16:07:59 - DEBUG - https://huggingface.co:443 "HEAD /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/custom_generate/generate.py HTTP/1.1" 404 0
Device set to use cuda:0
2025-09-10 16:08:09 - DEBUG - Pipeline loaded successfully
2025-09-10 16:08:09 - DEBUG - Pipeline setup complete
2025-09-10 16:08:09 - DEBUG - LLM Manager instance cached for reuse
2025-09-10 16:08:09 - INFO - Generated response with 0 words
2025-09-10 16:08:09 - INFO - Generated complete response with 0 words
2025-09-10 16:08:09 - WARNING - Rejected response at stage=first_attempt; reasons=['empty_response', 'offtopic/template']; preview=''
2025-09-10 16:08:58 - INFO - Generated response with 276 words
2025-09-10 16:08:58 - ERROR - Error in LLM response generation: Garbled/Surface/Offtopic response detected
2025-09-10 16:08:58 - WARNING - Retrying with minimal context
2025-09-10 16:08:58 - INFO - Generated response with 0 words
2025-09-10 16:08:58 - INFO - Generated complete response with 0 words
2025-09-10 16:08:58 - WARNING - Rejected response at stage=second_attempt; reasons=['empty_response', 'offtopic/template']; preview=''
2025-09-10 16:09:43 - INFO - Generated response with 307 words
2025-09-10 16:09:43 - ERROR - Rewrite after second attempt still invalid, using fallback
SUCCESS: Query processed

RESPONSE:
I apologize, but I'm having trouble generating a comprehensive response for your question about 'What is sustainability?'. This could be due to limited relevant information in the available sources or technical issues. Please try rephrasing your question, asking about a different topic, or contact support if the issue persists.

SOURCES USED:
  [1] Score: 0.000 - Key competencies in sustainability a reference framework for academic program development.pdf
  [2] Score: 0.000 - WORK-I~4.PDF
  [3] Score: 0.000 - Engineering_education_and_the_.pdf

Response length: 329 characters
